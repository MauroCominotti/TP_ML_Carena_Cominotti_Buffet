{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EE9Rlf0RUKhR"
      },
      "source": [
        "# TP 3 Machine Learning\n",
        "**Fecha y hora de entrega:** 30/05/2022 18:00\n",
        "\n",
        "*Para más información visitar [el TP en la wiki oficial de la materia](https://github.com/ucseml-team/machine-learning-course/wiki/TP3_2022)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xz0IG_tgLH0B"
      },
      "source": [
        "# **Fashion-MNIST Dataset**\n",
        "\n",
        "Se trata de un conjunto de datos de 60.000 imágenes en escala de grises de 28x28 de 10 categorías de moda, junto con un conjunto de prueba de 10.000 imágenes. Este conjunto de datos puede utilizarse como reemplazo de MNIST.\n",
        "\n",
        "<img src=\"https://tensorflow.org/images/fashion-mnist-sprite.png\"></img>\n",
        "\n",
        "\n",
        "**Las clases son:**\n",
        "\n",
        "Label   | Description\n",
        "--------|------------------\n",
        "0       | T-shirt/top - Remera/Top\n",
        "1       | Trouser - Pantalón\n",
        "2       | Pullover - Jersey\n",
        "3       | Dress - Vestido\n",
        "4       | Coat - Abrigo\n",
        "5       | Sandal - Sandalia\n",
        "6       | Shirt - Camisa\n",
        "7       | Sneaker - Zapatilla\n",
        "8       | Bag - Bolso\n",
        "9       | Ankle boot - Botas\n",
        "\n",
        "\n",
        "**Retorna**\n",
        "\n",
        "Tupla de matrices NumPy: (x_train, y_train), (x_test, y_test):\n",
        "\n",
        "**x_train**: matriz NumPy de uint8 de datos de imagen en escala de grises con formas (60000, 28, 28), que contiene los datos de train.\n",
        "\n",
        "**y_train**: matriz NumPy uint8 de etiquetas (enteros en el rango 0-9) con forma (60000,) para los datos de train.\n",
        "\n",
        "**x_test**: uint8 NumPy array de datos de imagen en escala de grises con forma (10000, 28, 28), que contiene los datos de test.\n",
        "\n",
        "**y_test**: matriz NumPy uint8 de etiquetas (enteros en el rango 0-9) con forma (10000,) para los datos de test."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0MziGzXM0uN"
      },
      "source": [
        "## **Cargando al Fashion-MNIST dataset**\n",
        "\n",
        "> **load_data** function\n",
        "\n",
        "\n",
        "```\n",
        "keras.datasets.fashion_mnist.load_data()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "*Para más información acerca del dataset visitar a [la página oficial de Fashion MNIST](https://keras.io/api/datasets/fashion_mnist/)*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_06eXq3RiviB"
      },
      "outputs": [],
      "source": [
        "# !pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1t-amysyks2"
      },
      "outputs": [],
      "source": [
        "# de python, para especificar rutas de archivos y directorios\n",
        "from pathlib import Path\n",
        "\n",
        "# lib para trabajar con arrays\n",
        "import numpy as np\n",
        "\n",
        "# lib que usamos para mostrar las imágenes\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.graph_objs as go\n",
        "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
        "from plotly import tools\n",
        "\n",
        "# libs que usamos para construir y entrenar redes neuronales, y que además tiene \n",
        "# utilidades para leer sets de imágenes\n",
        "\n",
        "# TensorFlow y tf.keras\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Input, Dropout, Convolution2D, MaxPooling2D, Flatten, Conv2D\n",
        "from keras.preprocessing.image import load_img, img_to_array, ImageDataGenerator\n",
        "from keras.utils.vis_utils import model_to_dot, plot_model\n",
        "\n",
        "# libs que usamos para tareas generales de machine learning. En este caso, métricas\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "# configuración para que las imágenes se vean dentro del notebook\n",
        "%matplotlib inline\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from IPython.display import SVG\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PQHtGpZLzXGT"
      },
      "outputs": [],
      "source": [
        "fashion_mnist = keras.datasets.fashion_mnist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1ooW_FESvZt"
      },
      "source": [
        "## Train y Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tVKW6zlR99Pe"
      },
      "outputs": [],
      "source": [
        "# lo vamos a estar usando seguido\n",
        "CLOTHES = \"Remera/Top\", \"Pantalón\", \"Jersey\", \"Vestido\", \"Abrigo\", \"Sandalia\", \"Camisa\", \"Zapatilla\", \"Bolso\", \"Botas\"\n",
        "\n",
        "# (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "(train_data, test_data) = fashion_mnist.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slOBolp8l8Nh"
      },
      "source": [
        "# Análisis exploratorio sobre el conjunto de datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pz8JqtTtvh4X"
      },
      "source": [
        "## Volumetría de los datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cSjYRNvjm9nR"
      },
      "outputs": [],
      "source": [
        "# Ver las dimensiones del dataset\n",
        "assert train_images.shape == (60000, 28, 28)\n",
        "assert test_images.shape == (10000, 28, 28)\n",
        "assert train_labels.shape == (60000,)\n",
        "assert test_labels.shape == (10000,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mO3WDNj6r714"
      },
      "outputs": [],
      "source": [
        "# Mostrar la cantidad de datos en train\n",
        "data_shape = [('train_images', len(train_images), train_images.shape),\n",
        "              ('train_labels', len(train_labels), train_labels.shape),\n",
        "              ('test_images', len(test_images), test_images.shape),\n",
        "              ('test_labels', len(test_labels), test_labels.shape)]\n",
        "pd.DataFrame(data_shape, columns=['Dataset Name', 'Cantidad de datos', 'Shape'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-vs6fRJlsBR-"
      },
      "outputs": [],
      "source": [
        "train_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvpNNZbQS516"
      },
      "source": [
        "## Estructura y tipo de las imágenes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l9gEKwQPxjWJ"
      },
      "outputs": [],
      "source": [
        "def print_dataset_images(dimensions, images, labels, isColorbar=False):\n",
        "  plt.figure(figsize=(10,10))\n",
        "  for i in range(dimensions*dimensions):\n",
        "      plt.subplot(dimensions,dimensions,i+1)\n",
        "      plt.xticks([])\n",
        "      plt.yticks([])\n",
        "      plt.imshow(images[i])\n",
        "      if isColorbar: \n",
        "        plt.colorbar()\n",
        "      plt.grid(False)\n",
        "      plt.title(\"La clase es: {} \\n {}\".format(labels[i], CLOTHES[labels[i]]))\n",
        "  plt.subplots_adjust(top=1.1, right=1)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P8WsfPrYJjal"
      },
      "outputs": [],
      "source": [
        "print_dataset_images(5, train_images, train_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CLDnL6UkwInl"
      },
      "outputs": [],
      "source": [
        "print_dataset_images(5, train_images, train_labels, True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IutiThqvswn"
      },
      "source": [
        "## Distribución de la variable a predecir\n",
        "\n",
        "Se puede observar que en ambos datasets, tanto train como test, los datos se encuentran completamente balanceados"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_pie_per_class(labels):\n",
        "  lista_train = np.zeros(10)\n",
        "  for label in labels: \n",
        "      lista_train[label] += 1 \n",
        "  plt.title(\"distibucion prendas en train\")\n",
        "  plt.pie(lista_train, labels=CLOTHES, autopct=\"%0.2f %%\");"
      ],
      "metadata": {
        "id": "9aV3UrK0v-iH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_count_per_class(yd):\n",
        "    ydf = pd.DataFrame(yd)\n",
        "    label_counts = ydf[0].value_counts() # Totales de cada clase\n",
        "    total_samples = len(yd) # Ejemplos totales\n",
        "\n",
        "    for i in range(len(label_counts)): # Contar numero de items en cada clase\n",
        "        label = CLOTHES[label_counts.index[i]]\n",
        "        count = label_counts.values[i]\n",
        "        percent = (count / total_samples) * 100\n",
        "        print(\"{:<20s}:   {} or {:.2f}%\".format(label, count, percent))"
      ],
      "metadata": {
        "id": "2CPFmds_wmQt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train"
      ],
      "metadata": {
        "id": "GZIZDBH4NbMh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "get_count_per_class(train_labels)"
      ],
      "metadata": {
        "id": "QM1O9w7oy4dn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gdtEyIQIuIBB"
      },
      "outputs": [],
      "source": [
        "plot_pie_per_class(train_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test"
      ],
      "metadata": {
        "id": "O3c7lQMONeSu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "get_count_per_class(test_labels)"
      ],
      "metadata": {
        "id": "8G9kjWHsvvOV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BWn4boFouIBB"
      },
      "outputs": [],
      "source": [
        "plot_pie_per_class(test_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBqmR92CwuZB"
      },
      "source": [
        "# Preprocesado del dataset a trabajar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1JKQ4cvsw0U3"
      },
      "outputs": [],
      "source": [
        "train_images = train_images / 255.0\n",
        "test_images = test_images / 255.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24ndyXihw64X"
      },
      "outputs": [],
      "source": [
        "print_dataset_images(5, train_images, train_labels, True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9WPW5sOS_1K"
      },
      "source": [
        "# Machine Learning "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrvFvAatuIBD"
      },
      "source": [
        "## Funciones útiles: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Jh5LSjbuIBD"
      },
      "outputs": [],
      "source": [
        "# Función que dibuja las curvas\n",
        "def curvas(model):\n",
        "    plt.plot(model.history['accuracy'], label='train')\n",
        "    plt.plot(model.history['val_accuracy'], label='test')\n",
        "    plt.title('Accuracy over train epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(loc='upper left')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def crear_curvas(x, y, ylabel, color):\n",
        "        trace = go.Scatter(\n",
        "            x = x,y = y,\n",
        "            name=ylabel,\n",
        "            marker=dict(color=color),\n",
        "            mode = \"markers+lines\",\n",
        "            text=x\n",
        "        )\n",
        "        return trace\n",
        "    \n",
        "def plot_accuracy_and_loss(model):\n",
        "    hist = model.history\n",
        "    acc = hist['accuracy']\n",
        "    val_acc = hist['val_accuracy']\n",
        "    loss = hist['loss']\n",
        "    val_loss = hist['val_loss']\n",
        "    epochs = list(range(1,len(acc)+1))\n",
        "    \n",
        "    trace_ta = crear_curvas(epochs,acc,\"Training accuracy\", \"Green\")\n",
        "    trace_va = crear_curvas(epochs,val_acc,\"Validation accuracy\", \"Red\")\n",
        "    trace_tl = crear_curvas(epochs,loss,\"Training loss\", \"Blue\")\n",
        "    trace_vl = crear_curvas(epochs,val_loss,\"Validation loss\", \"Magenta\")\n",
        "\n",
        "    fig = tools.make_subplots(rows=1,cols=2, subplot_titles=('Training and validation accuracy',\n",
        "                                                             'Training and validation loss'))\n",
        "    fig.append_trace(trace_ta,1,1)\n",
        "    fig.append_trace(trace_va,1,1)\n",
        "    fig.append_trace(trace_tl,1,2)\n",
        "    fig.append_trace(trace_vl,1,2)\n",
        "    fig['layout']['xaxis'].update(title = 'Epoch')\n",
        "    fig['layout']['xaxis2'].update(title = 'Epoch')\n",
        "    fig['layout']['yaxis'].update(title = 'Accuracy', range=[0,1])\n",
        "    fig['layout']['yaxis2'].update(title = 'Loss', range=[0,1])\n",
        "\n",
        "    \n",
        "    iplot(fig, filename='accuracy-loss')"
      ],
      "metadata": {
        "id": "-ZhUFehNbkFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8CBjzOouIBD"
      },
      "source": [
        "## Modelos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-SnIivOuIBD"
      },
      "source": [
        "### MLP 1:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "02-3gnj1uIBE"
      },
      "outputs": [],
      "source": [
        "model_mlp_1 = Sequential([\n",
        "    Flatten(input_shape=(28,28)),\n",
        "\n",
        "    Dense(10, activation='relu'),\n",
        "\n",
        "    Dense(len(CLOTHES), activation='softmax'),\n",
        "])\n",
        "\n",
        "model_mlp_1.compile(\n",
        "    # Variable por descenso de gradiente a utilizar:\n",
        "    optimizer='adam',\n",
        "    # Función de error a utilizar:\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy',],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tabD7Qr9uIBE"
      },
      "outputs": [],
      "source": [
        "# Entrenamos:\n",
        "train_model_mlp_1 = model_mlp_1.fit(\n",
        "    train_images,\n",
        "    train_labels,\n",
        "    epochs=5,\n",
        "    batch_size=10,\n",
        "    validation_data=(test_images, test_labels)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UIhtdjqZuIBE"
      },
      "outputs": [],
      "source": [
        "curvas(train_model_mlp_1)\n",
        "plot_accuracy_and_loss(train_model_mlp_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SMpS7FCsuIBE"
      },
      "outputs": [],
      "source": [
        "mejor_puntaje = max(train_model_mlp_1.history['val_accuracy'])\n",
        "mejor_epoca = np.array(train_model_mlp_1.history['val_accuracy']).argmax()+1\n",
        "print('La mejor precisión en test fue %f en la epoca %i' % (mejor_puntaje,mejor_epoca))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeTX88HguIBE"
      },
      "source": [
        "### MLP 2:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f0nVcwPpuIBE"
      },
      "outputs": [],
      "source": [
        "model_mlp_2 = Sequential([\n",
        "    Flatten(input_shape=(28,28)),\n",
        "    Dense(200, activation='relu'),\n",
        "    Dropout(0.1),\n",
        "    Dense(200, activation='relu'),\n",
        "    Dropout(0.1),\n",
        "    Dense(200, activation='relu'),\n",
        "    Dropout(0.1),\n",
        "    Dense(100, activation='relu'),\n",
        "    Dropout(0.1),\n",
        "    Dense(50, activation='relu'),\n",
        "    Dropout(0.1),\n",
        "    Dense(len(CLOTHES), activation='softmax'),\n",
        "])\n",
        "\n",
        "model_mlp_2.compile(\n",
        "    # Variable por descenso de gradiente a utilizar:\n",
        "    optimizer='adam',\n",
        "    # Función de error a utilizar:\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy',],\n",
        ")\n",
        "\n",
        "model_mlp_2.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tqAzqtUAuIBF"
      },
      "outputs": [],
      "source": [
        "# Entrenamos:\n",
        "train_model_mlp_2 = model_mlp_2.fit(\n",
        "    train_images,\n",
        "    train_labels,\n",
        "    epochs=20,\n",
        "    batch_size=300,\n",
        "    validation_data=(test_images, test_labels)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gzLuPfWTuIBF"
      },
      "outputs": [],
      "source": [
        "curvas(train_model_mlp_2)\n",
        "plot_accuracy_and_loss(train_model_mlp_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lWoXQC_ruIBF"
      },
      "outputs": [],
      "source": [
        "mejor_puntaje = max(train_model_mlp_2.history['val_accuracy'])\n",
        "mejor_epoca = np.array(train_model_mlp_2.history['val_accuracy']).argmax()+1\n",
        "print('La mejor precisión en test fue %f en la epoca %i' % (mejor_puntaje,mejor_epoca))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aif_LDHBuIBF"
      },
      "source": [
        "Vemos que la precisión en el ds de train aumenta con el número de épocas. La precisión del ds de test también aumenta inicialmente con el número de épocas. Sin embargo, en algún momento puede comenzar a disminuir debido al sobreentrenamiento. De hecho comenzamos a ver esto en el momento en la epoca 16, donde la precisión fue de 89% y luego comienza a disminuir."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CNN 1:"
      ],
      "metadata": {
        "id": "kvXoK797vJto"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N25Nten-uIBG"
      },
      "outputs": [],
      "source": [
        "model_cnn_1 = Sequential([\n",
        "    Convolution2D(32, kernel_size=(3, 3),\n",
        "                 activation='relu',\n",
        "                 kernel_initializer='he_normal',\n",
        "                 input_shape=(28, 28, 1)), # input_shape=(filas en las img, cols en las img)\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Convolution2D(64, kernel_size=(3, 3), activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Convolution2D(128, (3, 3), activation='relu'),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(len(CLOTHES), activation='softmax'),\n",
        "])\n",
        "\n",
        "model_cnn_1.compile(\n",
        "              optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy', ])\n",
        "\n",
        "model_cnn_1.summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_model(model_cnn_1, to_file='model.png')\n",
        "SVG(model_to_dot(model_cnn_1, dpi=65).create(prog='dot', format='svg'))"
      ],
      "metadata": {
        "id": "NuWA34JpQD2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model_cnn_1 = model_cnn_1.fit(\n",
        "                train_images,\n",
        "                train_labels,\n",
        "                epochs=40,\n",
        "                batch_size=300,\n",
        "                verbose=1,\n",
        "                validation_data=(test_images, test_labels),\n",
        "                )"
      ],
      "metadata": {
        "id": "GTcT9d9LSfaE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score = model_cnn_1.evaluate(test_images, test_labels, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "metadata": {
        "id": "KrPgRnGiaqY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_accuracy_and_loss(train_model_cnn_1)"
      ],
      "metadata": {
        "id": "OAA228vqcBen"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CNN 2: Agregando capas Dropout"
      ],
      "metadata": {
        "id": "eb0ssBwgfuAl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_cnn_2 = Sequential([\n",
        "    Convolution2D(32, kernel_size=(3, 3),\n",
        "                 activation='relu',\n",
        "                 kernel_initializer='he_normal',\n",
        "                 input_shape=(28, 28, 1)),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Dropout(0.25),\n",
        "    Convolution2D(64, kernel_size=(3, 3), activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Dropout(0.25),\n",
        "    Convolution2D(128, (3, 3), activation='relu'),\n",
        "    Dropout(0.25),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.25),\n",
        "    Dense(len(CLOTHES), activation='softmax'),\n",
        "])\n",
        "\n",
        "model_cnn_2.compile(\n",
        "              optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy', ])\n",
        "\n",
        "model_cnn_2.summary()"
      ],
      "metadata": {
        "id": "8fMBoJ6idCaQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_model(model_cnn_2, to_file='model.png')\n",
        "SVG(model_to_dot(model_cnn_2, dpi=65).create(prog='dot', format='svg'))"
      ],
      "metadata": {
        "id": "fbfGHkiBf9Sx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model_cnn_2 = model_cnn_2.fit(\n",
        "                train_images,\n",
        "                train_labels,\n",
        "                epochs=40,\n",
        "                batch_size=300,\n",
        "                verbose=1,\n",
        "                validation_data=(test_images, test_labels),\n",
        "                )"
      ],
      "metadata": {
        "id": "JECjKfDdf-n3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_accuracy_and_loss(train_model_cnn_2)"
      ],
      "metadata": {
        "id": "JXAOgZeBgJ_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score = model_cnn_2.evaluate(test_images, test_labels, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "metadata": {
        "id": "GtH_GsBxgHjC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predicciones con CNN 2"
      ],
      "metadata": {
        "id": "roirzhLOgWCh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# predicted values\n",
        "y_pred_enc = model_cnn_2.predict(test_images)\n",
        "\n",
        "# decoding predicted values\n",
        "y_pred = [np.argmax(i) for i in y_pred_enc]\n",
        "\n",
        "print(y_pred_enc[0])\n",
        "print(y_pred[0])"
      ],
      "metadata": {
        "id": "R2ENyHH0tDDM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# predicted targets of each images\n",
        "fig, ax = plt.subplots(figsize=(18, 8))\n",
        "for ind, row in enumerate(test_images[:8]):\n",
        "    plt.subplot(2, 4, ind+1)\n",
        "    predict_index = np.argmax(y_pred[ind])\n",
        "    true_index = np.argmax(test_labels[ind])\n",
        "    plt.title(\"{} ({})\".format(CLOTHES[y_pred[ind]], \n",
        "                                  CLOTHES[test_labels[ind]]),\n",
        "                                  color=(\"green\" if predict_index == true_index else \"red\"))\n",
        "    img = row.reshape(28, 28)\n",
        "    fig.suptitle('Predicted values', fontsize=24)\n",
        "    plt.axis('off')\n",
        "    plt.imshow(img, cmap='cividis')"
      ],
      "metadata": {
        "id": "2R4wa8MPtMcM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(test_labels, y_pred))"
      ],
      "metadata": {
        "id": "UchoFBUJseQ6"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "tp3_fashion_mnist.ipynb",
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}