{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# https://stackoverflow.com/questions/21971449/how-do-i-increase-the-cell-width-of-the-jupyter-ipython-notebook-in-my-browser\n",
        "from IPython.core.display import display, HTML\n",
        "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
        "display(HTML(\"<style>.output_result { max-width:100% !important; }</style>\"))\n",
        "display(HTML(\"<style>.prompt { display:none !important; }</style>\"))"
      ],
      "metadata": {
        "id": "zH_Kd00CwNdv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trabajo Práctico 2: Entrenamiento y evaluación de modelos\n",
        "---"
      ],
      "metadata": {
        "id": "fggOEw1ZsPtX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fecha y hora de entrega máxima:\n",
        "09/05/2022 18:00"
      ],
      "metadata": {
        "id": "sNVf73dvsZMx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset \"Datos de clientes del banco\"\n",
        "Los datos están relacionados con campañas de marketing directo (llamadas telefónicas) de una institución bancaria portuguesa. El objetivo de la clasificación es predecir si el cliente suscribirá un depósito a plazo.\n",
        "\n",
        "<img src=\"https://storage.googleapis.com/kaggle-datasets-images/864595/1473402/1f559c7d6d646d0a5f24c1847fb10225/dataset-cover.jpg?t=2020-09-08-19-15-14\"></img>"
      ],
      "metadata": {
        "id": "mJvBcdjAvLC0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3bf5c98e"
      },
      "outputs": [],
      "source": [
        "# Import dependencies\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly\n",
        "import plotly.express as px\n",
        "import pandas as pd\n",
        "import sklearn_pandas\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn_pandas import DataFrameMapper\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, roc_curve, roc_auc_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If we import the dataset from the csv file we see we have values with the value \"unknown\"\n",
        "dataset_original = pd.read_csv(\"BankCustomerData.csv\")\n",
        "\n",
        "# We copy the dataset so we don't change directly the original dataset after working on it\n",
        "ds = dataset_original\n",
        "\n",
        "# Return the first five rows of the DataFrame\n",
        "ds.head()"
      ],
      "metadata": {
        "id": "dBYG8LKdwkgi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To replace these values with NaN, we must provide a list with all missing value formats\n",
        "missing_value_formats = [\"unknown\", \"n.a.\",\"?\",\"NA\",\"n/a\", \"na\", \"--\"]\n",
        "# missing_value_formats = [\"n.a.\",\"?\",\"NA\",\"n/a\", \"na\", \"--\"]\n",
        "ds = pd.read_csv(\"BankCustomerData.csv\", na_values = missing_value_formats)\n",
        "\n",
        "# Display the firsts and lasts lines of the file\n",
        "ds"
      ],
      "metadata": {
        "id": "vvqmm3FRx86O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ds.dropna(inplace=True)\n",
        "# ds.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Display the firsts and lasts lines of the file\n",
        "ds"
      ],
      "metadata": {
        "id": "SaTelYGd09-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Elección de una métrica\n",
        "La métrica que utilizaremos es Accuracy, debido a que permite medir el porcentaje de casos acertados en la predicción, si"
      ],
      "metadata": {
        "id": "UfnZWo5osCa0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Entrenamiento de Modelos\n",
        "Elegimos los siguientes 3 modelos para entrenar:\n",
        "- Logistic Regression\n",
        "- Neural Networks MLP\n",
        "- KNN"
      ],
      "metadata": {
        "id": "63QpN4arysll"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Logistic Regression"
      ],
      "metadata": {
        "id": "XeMJjqYEThF_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dividimos el dataset en train (60%), test (20%) y validation (20%)\n",
        "train, not_train = train_test_split(ds, test_size=0.4, random_state=42)\n",
        "validation, test = train_test_split(not_train, test_size=0.5, random_state=42)\n",
        "\n",
        "ds[\"term_deposit\"] = ds.term_deposit.replace(['no', 'yes'], [0,1])\n",
        "ds[\"housing\"] = ds.housing.replace(['no', 'yes'], [0,1])\n",
        "ds[\"loan\"] = ds.loan.replace(['no', 'yes'], [0,1])\n",
        "\n",
        "mapper_scaller = DataFrameMapper([\n",
        "    (['age'],[MinMaxScaler()]),\n",
        "    (['loan'],None),\n",
        "    (['term_deposit'],None),\n",
        "    (['housing'],None),\n",
        "    (['job'],[OneHotEncoder()]),\n",
        "    (['education'],[OneHotEncoder()]),\n",
        "    (['balance'],[MinMaxScaler()])\n",
        "])\n",
        "\n",
        "mapper_scaller.fit(train)\n"
      ],
      "metadata": {
        "id": "I6bm-gzZ5fO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We selected this parameters to prevent overfitting and because as we saw in \n",
        "# the previous TP, these might have a correlation to term_deposit\n",
        "ds_some_rows = ds[['loan', 'housing', 'job', 'education', 'age', 'balance', 'term_deposit']]\n",
        "x = ds_some_rows.iloc[:, :-1].values\n",
        "y = ds_some_rows.iloc[:, -1].values\n",
        "ds_some_rows"
      ],
      "metadata": {
        "id": "15ZPnPP-5LFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x"
      ],
      "metadata": {
        "id": "akQpqzGq-1a7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ct = ColumnTransformer(transformers=[('encode', \n",
        "                                      OneHotEncoder(), \n",
        "                                      [0, 1, 2, 3])], \n",
        "                       remainder='passthrough')\n",
        "x = np.array(ct.fit_transform(x))\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y)"
      ],
      "metadata": {
        "id": "AFz5FLQy5kQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x"
      ],
      "metadata": {
        "id": "8FQ4seCA7-S1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "id": "fJIAclKL7-_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting data to training and test set \n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)"
      ],
      "metadata": {
        "id": "c3lPR4GM6NTc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying logistic regression to the data\n",
        "logistic_regression = LogisticRegression(random_state=0)\n",
        "logistic_regression.fit(x_train, y_train)"
      ],
      "metadata": {
        "id": "Dn3sqcvJ_4e7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Then predict results \n",
        "y_pred = logistic_regression.predict(x_test)\n",
        "print(y_pred)"
      ],
      "metadata": {
        "id": "YW9WV6bwATDr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking my correct and in-correct predictions using the confusion_matrix and \n",
        "# then checking the accuracy of the model with the accuracy_score\n",
        "confusion_matrix_result = confusion_matrix(y_pred, y_test)\n",
        "print(\"Confusion Matrix:\")\n",
        "confusion_matrix_result"
      ],
      "metadata": {
        "id": "ZnCwsO_9AeF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Accuracy:\",accuracy_score(y_test, y_pred))\n",
        "print(\"Precision:\",precision_score(y_test, y_pred))\n",
        "print(\"Recall:\",recall_score(y_test, y_pred))\n",
        "# TODO: See why precision and recall is zero"
      ],
      "metadata": {
        "id": "zuBkZVU_M3Hv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Evaluation using Confusion Matrix\n",
        "A confusion matrix is a table that is used to evaluate the performance of a classification model. You can also visualize the performance of an algorithm. The fundamental of a confusion matrix is the number of correct and incorrect predictions are summed up class-wise.\n",
        "\n",
        "The dimension of this matrix is 2*2 because this model is binary classification. You have two classes 0 and 1. Diagonal values represent accurate predictions, while non-diagonal elements are inaccurate predictions.\n",
        "\n",
        "**In the output, 7733 and 0 are actual predictions, and 1 and 794 are incorrect predictions.**"
      ],
      "metadata": {
        "id": "1aPz9gIMMnY-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class_names=[0,1] # name  of classes\n",
        "fig, ax = plt.subplots()\n",
        "tick_marks = np.arange(len(class_names))\n",
        "plt.xticks(tick_marks, class_names)\n",
        "plt.yticks(tick_marks, class_names)\n",
        "# create heatmap\n",
        "sns.heatmap(pd.DataFrame(confusion_matrix_result), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
        "ax.xaxis.set_label_position(\"top\")\n",
        "plt.tight_layout()\n",
        "plt.title('Confusion matrix', y=1.1)\n",
        "plt.ylabel('Actual label')\n",
        "plt.xlabel('Predicted label')"
      ],
      "metadata": {
        "id": "XUJD1-C3OXtZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ROC Curve\n",
        "Receiver Operating Characteristic(ROC) curve is a plot of the true positive rate against the false positive rate. It shows the tradeoff between sensitivity and specificity."
      ],
      "metadata": {
        "id": "aqXWoPr1Np8m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_proba = logistic_regression.predict_proba(x_test)[::,1]\n",
        "fpr, tpr, _ = roc_curve(y_test,  y_pred_proba)\n",
        "auc = roc_auc_score(y_test, y_pred_proba)\n",
        "plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n",
        "plt.legend(loc=4)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "V8Uu53UGNRiz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**AUC score for the case is 0.60.**\n",
        "\n",
        "AUC score 1 represents perfect classifier, and 0.5 represents a worthless classifier."
      ],
      "metadata": {
        "id": "4FPRL3NfNwaQ"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "name": "tp2_experimentation.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}